{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 書くこと"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- [参考のノートブック](https://www.kaggle.com/artgor/segmentation-in-pytorch-using-convenient-tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# はじめに\n",
    "\n",
    "本文書は，日本コンピュータ外科学会によって開催されたJSCAS AI Challengeという機械学習のコンペティションに参加したときの記録です．\n",
    "\n",
    "https://www.jscas.org/business/2020/09/c83412be4feb3ca543c74d759fee400c073cda07.html\n",
    "\n",
    "\n",
    "この文書では大きく分けて2つの話題，コンペティションまでに行ったこと，コンペティション後に他の人の発表を参考に改善を試みたこと，について書きます．\n",
    "\n",
    "コンペティションの発表までに行ったこととしては，モデルの選定，損失関数の調査，パラメータチューニングなどがあります．\n",
    "\n",
    "また，アンサンブル学習を行い，結果的にこれが最も効果のある学習方法でした．\n",
    "今回は主にこのアンサンブル学習について実際にどのように行ったのか書きます．\n",
    "\n",
    "コンペティション後には，他の人の発表を参考にしながら改善を試みました．\n",
    "こちらについては結果的に大きな精度の向上は達成できませんでしたが，\n",
    "いくつか実験を行ったので記録として書きます．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# コンペティション概要\n",
    "\n",
    "先に述べた通り，参加したコンペティションは，日本コンピュータ外科学会によって開催されたJSCAS AI Challengeという名前の機械学習コンペティションです．\n",
    "\n",
    "コンペティションで要求されるタスクは，内視鏡外科手術画像の中から術具の先端部分を自動で識別せよ，というセマンティックセグメンテーションタスクです．\n",
    "\n",
    "具体的には超音波凝固切開装置と呼ばれる医療器具が画像に写っており，\n",
    "切開を行う先端部分を認識することが求められていることです．\n",
    "\n",
    "また，この超音波凝固切開装置には新旧の二種類が存在し，それを扱うか否かによって以下の2つのタスクレベルに分けられていました．\n",
    "\n",
    "- レベル1：新旧の区別無く術具の先端を認識\n",
    "- レベル2：新旧の区別（クラスタリング）を行いかつ，術具の先端を認識\n",
    "\n",
    "今回私はレベル1のみを行いました（機械学習初心者でクラスタリングまで手が回らなかったためです）．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価指標\n",
    "\n",
    "評価指標にはF値を使います．\n",
    "セマンティックセグメンテーションでは各ピクセルごとに正誤判定ができます．\n",
    "正誤判定において正と判定されたもののうち実際に正であるものの割合をprecision,\n",
    "実際に正であるもののうち正と判定されたものの割合をrecallと呼び，\n",
    "precisionとrecallの調和平均をF値と呼びます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセット\n",
    "\n",
    "データセットは以下の通りです．\n",
    "\n",
    "- トレーニング画像\n",
    "    - 325枚\n",
    "- テスト画像\n",
    "    - 75枚\n",
    "- 検証画像\n",
    "    - 150枚（新：75枚，旧：75枚）\n",
    "    - セグメンテーションの難易度毎にeasy, middle, hardに分類される\n",
    "    \n",
    "すべての画像には対応するラベル画像があり，\n",
    "ラベル画像は背景(0)，旧(1)，新(2)が各ピクセルに割り当てられています．\n",
    "    \n",
    "ラベルを含む画像データはJSCASのページに公開されています．\n",
    "http://bit.ly/ai20data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手法\n",
    "\n",
    "書くこと\n",
    "- どのようにデータの前処理をおこなったか\n",
    "- どのような識別器を用いたか\n",
    "    - モデル\n",
    "    - 損失関数\n",
    "    - 最適化アルゴリズム\n",
    "- どのように術具の領域を推定したか\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理\n",
    "\n",
    "トレーニング画像325枚に対して，精度向上を目的として,いくつかの前処理を行う\n",
    "- mixup\n",
    "- Data Augumentation(DA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mixup\n",
    "\n",
    "二枚の画像を割合で混ぜる  \n",
    "$\\lambda \\times x_1 + (1 - \\lambda) \\times x_2$  \n",
    "$\\lambda \\sim Beta(\\alpha, \\alpha)$\n",
    "今回$\\alpha$は0.4にした"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataAugmentation(以下DA)\n",
    "\n",
    "- 平行移動，反転などのアフィン変換\n",
    "- 明るさ\n",
    "- 色の濃度の変更\n",
    "\n",
    "具体的なコードを貼る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習モデル\n",
    "\n",
    "- DeepLabV3+を使用\n",
    "    - 参考実装のu-netより学習速度が早い\n",
    "    \n",
    "DeepLabv3+はGoogleが2018年に出したモデルで，空間ピラミッドプーリング\n",
    "モジュールと、Encoder-Decoder モデルの利点を組み合わせたモデルです．\n",
    "詳しくは論文やQiitaのまとめなどを参照．\n",
    "https://qiita.com/mine820/items/14e7c556b358dbc4ee9a\n",
    "\n",
    "また，セマンティックセグメンテーションの代表的なモデルU-netとDeepLabv3+で学習速度を比較するとDeepLabv3+のほうが早く，高速にイテレーションを回すことができます．\n",
    "\n",
    "参考にしたコードからの変更点としては，ドロップアウト率を0.5に変更したこと，があります．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失関数\n",
    "\n",
    "損失関数については，いくつかの調査を行いました．\n",
    "セマンティックセグメンテーションでは，認識対象の領域と背景との比率が大きく異る場合があります．\n",
    "今回扱うデータにおいても，術具の先端という非常に小さい領域が認識対象です．\n",
    "そのような場合には一般的なbinary cross entroyよりもセマンティックセグメンテーションに特化した損失関数を用いると，精度が向上する場合があります．\n",
    "\n",
    "今回は以下のような損失関数を検討しました．\n",
    "\n",
    "- binary crossentropy\n",
    "- sigmoid focal loss\n",
    "- focal tversky\n",
    "- boundary loss\n",
    "\n",
    "結果としてfocal tverskyを基本的に使用しました．\n",
    "後述するアンサンブル学習の関係で，一部の識別器ではboundary loss関数をfocal tverskyと併用している場合もあります．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## アンサンブル学習\n",
    "\n",
    "複数の弱識別器を組み合わせて学習・推論を行う機械学習の手法をアンサンブル学習といいます．\n",
    "\n",
    "今回は複数の分類器の出力の平均を取ることでより良い分類（セグメンテーション）結果を得る，という形でアンサンブル学習の考え方を取り入れています．\n",
    "\n",
    "図で説明する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コンペティションに提出した際に使った識別器は以下の３つです．\n",
    "- DA有り($y_1$)\n",
    "- DA無し($y_2$)\n",
    "- 損失関数の違い($y_3$)\n",
    "    \n",
    "$(y_1 + y_2 + y_3) / 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 評価\n",
    "\n",
    "実際の推定結果を比較して，アンサンブル学習の効果を確かめます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DeepLabv3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-36753ad8ccfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# create model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepLabv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepLabv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepLabv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DeepLabv3' is not defined"
     ]
    }
   ],
   "source": [
    "# DA\n",
    "\n",
    "# create model\n",
    "model1 = DeepLabv3()\n",
    "model2 = DeepLabv3()\n",
    "model3 = DeepLabv3()\n",
    "\n",
    "# training\n",
    "model1.train()\n",
    "model2.train()\n",
    "model3.train()\n",
    "\n",
    "# estimation\n",
    "result1 = predict(model1)\n",
    "result2 = predict(model2)\n",
    "result3 = predict(model3)\n",
    "result = (result1+result2+result3)/3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習した識別器の性能を確かめるために，テストデータを用いて，識別器の推定精度を算出します．\n",
    "テストデータは識別難易度ごとにeasy,middle,hardに分かれており，それぞれについて推定精度を出します．\n",
    "3つの識別器それぞれの推定結果とアンサンブル学習の結果を図示しています．\n",
    "(hardについては，認識することが困難であったため，図から除外しています．)\n",
    "\n",
    "![result](./jscas_ai_challenge_2020_data/compare4.png)\n",
    "\n",
    "- FT+DA（青色）    \n",
    "- FT（橙色）       \n",
    "- FT+BL（緑色）    \n",
    "- ensenbled（赤色）\n",
    "\n",
    "青色のグラフはfocal tversky + data augmentation,\n",
    "橙色のグラフはfocal tversky,\n",
    "緑色のグラフはfocal tversky + boundary loss,\n",
    "赤色のグラフはアンサンブル学習の結果，\n",
    "となっています．\n",
    "\n",
    "アンサンブル学習に用いる識別器の選択は，今回はヒューリスティックに行いました．\n",
    "\n",
    "\n",
    "アンサンブル学習によってすべての難易度で二番目の精度を達成していることがわかります．\n",
    "\n",
    "それぞれの識別器に得意不得意なデータがありますが，アンサンブル学習を行うことによって，平均的に優秀な識別器を作成可能である，ということがわかりました．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 評価の結果に基づいた改良\n",
    "\n",
    "ここからはコンペティション発表後に行った実験について，いくつか紹介します．\n",
    "結論から書くと識別性能に大きく寄与するような学習方法は発見できませんでしたが，記録として残しておこうと思います．\n",
    "（特に識別できていなかったhardの画像を識別可能になることを期待したのですが，うまくいきませんでした）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて，行ったこととしては，\n",
    "\n",
    "- fog, DefocusBlurなどの画像処理をDAとして行う\n",
    "- RGBチャンネルに加えて，RGBチャンネルに前処理を施したデータを4つ目以降の特徴量として扱う\n",
    "- 上下反転のみでDAを行う\n",
    "\n",
    "といったものがあります．\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fog, DefocusBlurなどの画像処理をDA\n",
    "\n",
    "セグメンテーション精度が悪かった画像として，切開で焼き切るときに発生する煙で曇っていたり，カメラのピントがずれているものが多く見られました．\n",
    "とりわけテストデータの識別難易度hardはそのような画像が多く，これが識別困難の原因である可能性があります．\n",
    "\n",
    "そのような画像をDAで生成して識別器を訓練することで精度が向上するのではないだろうか，というのがこの試みです．\n",
    "\n",
    "DAにはimgaugライブラリを用いました．\n",
    "Fog(煙っぽい処理)やDefocusBlur(ピントのズレ)関数が用意されていて，簡単に画像を加工することができます．\n",
    "\n",
    "具体例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa\n",
    "\n",
    "da_list = iaa.Sequential([\n",
    "    iaa.imgcorruptlike.Fog(severity=2), # kemuri\n",
    "    iaa.imgcorruptlike.DefocusBlur(severity=2), # camera no pinto\n",
    "])\n",
    "\n",
    "# 実際に画像を加工してみる\n",
    "# 画像を出力するコード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Sequential` は 順番に処理を適用します．\n",
    "他にも渡した処理のうちいくつかを適用する`SomeOf`や，\n",
    "一つだけ適用する`OneOf`など便利な関数があります．\n",
    "\n",
    "結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データに合わせた特徴量の追加\n",
    "\n",
    "内視鏡手術画像の特徴として内臓からなる赤色が多くを占めるというものがあります．\n",
    "対して術具先端は黒に近い色で構成されているため，RGB空間でうまく分離できるのではないかと考えました．\n",
    "RGBチャネルのみを入力特徴量として扱うことで，内臓と術具先端を分けることができるのが理想ですが，実際にはそうでないこともあるため，明示的に新しい特徴量を導入することでうまくいかないかという試みです．\n",
    "\n",
    "今回は目的の処理を行うためにopencvのpythonライブラリを使いました．\n",
    "`threshold`という関数があり閾値処理をして画像の二値化ができます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_one_channel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-97e18ef904d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_one_channel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTHRESH_BINARY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'image_one_channel' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "cv2.threshold(image_one_channel, 100, 255, cv2.THRESH_BINARY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "渡す画像は0~255の値を持っています．\n",
    "ピクセル値が第二引数で指定した値以上の場合，そのピクセル値を第三引数の値にします．\n",
    "第四引数は数値の切り上げ方のオプションで，グラデーションで変化させることもできます．\n",
    "詳しくはこのページを見てください．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上下反転のDA\n",
    "\n",
    "実際にどのようなDAが効果的なのか，という疑問から試してみた一例です．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まとめ\n",
    "\n",
    "術具先端の識別を機械学習を用いて行った\n",
    "\n",
    "効果的だった手法はアンサンブル手法\n",
    "\n",
    "識別が困難な画像を模倣するようなDAをいくつか試したが，精度改善は見られなかった"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#　使用するモデル選択\n",
    "model = Deeplabv3(weights=None, input_shape=(176, 320, 3), OS=8, classes=1, backbone='xception', activation='sigmoid', dropout_rate=0.5)\n",
    "\n",
    "# モデル表示\n",
    "import keras\n",
    "from IPython.display import Image\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png', show_shapes=True)\n",
    "Image(retina=True, filename='model.png')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
